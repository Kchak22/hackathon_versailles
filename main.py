# =============================================================================
# ÉTAPE 1 : IMPORTS, CONFIGURATION ET DÉFINITION DE L'ÉTAT
# =============================================================================

import os
import json
import uuid
from dotenv import load_dotenv
from typing import List, Dict, Any, Annotated, Optional, Literal
from typing_extensions import TypedDict

# Imports LangChain/LangGraph
from mistralai import Mistral # Utilisation du client Mistral direct
from langchain_core.messages import HumanMessage, AIMessage
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import InMemorySaver

# --- Configuration ---
load_dotenv()
MISTRAL_API_KEY = os.getenv("MISTRAL_API_KEY")
if not MISTRAL_API_KEY:
    raise ValueError("MISTRAL_API_KEY is not set in the environment variables.")

# IDs des agents pré-configurés sur la plateforme Mistral
AGENT_ID_EXTRACTOR = os.getenv("MISTRAL_AGENT_ID_EXTRACTOR") 
AGENT_ID_QUESTIONER = os.getenv("MISTRAL_AGENT_ID_QUESTIONER") 

if not AGENT_ID_EXTRACTOR or not AGENT_ID_QUESTIONER:
    raise ValueError("Please set MISTRAL_AGENT_ID_EXTRACTOR and MISTRAL_AGENT_ID_QUESTIONER in your .env file.")

# Client Mistral
client = Mistral(api_key=MISTRAL_API_KEY)
print("Mistral client initialized.")


# --- Définition de l'État ---
# Correspond à la sortie JSON de l'AGENT_ID_1
class UserProfile(TypedDict, total=False):
    language: str
    time_window: Dict
    interests: List[str]
    constraints: List[str]
    meals_services: List[str]
    tickets_logistics: List[str]
    missing_fields: List[str]

class GraphState(TypedDict):
    """L'état global de notre application de chat."""
    messages: Annotated[list, add_messages]
    user_profile: UserProfile
    question_attempts: int

# =============================================================================
# ÉTAPE 2 & 3 : DÉFINITION DES NŒUDS (WRAPPERS D'AGENTS MISTRAL)
# =============================================================================

def information_extractor_node(state: GraphState) -> dict:
    """
    Ce nœud appelle l'Agent Mistral AGENT_ID_1 pour extraire les informations.
    C'est un wrapper autour de la logique de votre coéquipier.
    """
    print("--- NODE: Information Extractor (calling Mistral Agent API) ---")
    
    # On prend le dernier message utilisateur comme entrée
    last_user_message = state['messages'][-1].content
    
    try:
        resp = client.beta.conversations.start(
            agent_id=AGENT_ID_EXTRACTOR,
            inputs=[{"role": "user", "content": last_user_message}],
            store=False  # Important: LangGraph gère l'état, pas Mistral
        )
        
        text_response = resp.outputs[0].content if resp.outputs else ""
        extracted_data = json.loads(text_response)
        print(f"   > Profile data extracted by Agent 1: {extracted_data}")
        
        return {"user_profile": extracted_data}
        
    except Exception as e:
        print(f"   > Error calling Mistral Agent 1: {e}")
        return {"user_profile": {"missing_fields": ["all"]}}


def ask_clarification_question_node(state: GraphState) -> dict:
    """
    Ce nœud appelle l'Agent Mistral AGENT_ID_2 pour générer une question.
    """
    print("--- NODE: Clarification Question (calling Mistral Agent API) ---")
    
    profile = state.get('user_profile', {})
    
    # On reconstruit l'objet d'entrée exactement comme dans le script 
    input_for_agent_2 = {
        "language": profile.get('language'),
        "fields_to_ask": profile.get('missing_fields', []),
        "interests": profile.get('interests'),
        "constraints": profile.get('constraints'),
        "meals_services": profile.get('meals_services'),
        "tickets_logistics": profile.get('tickets_logistics'),
        "time_window": profile.get('time_window'),
    }
    
    # On convertit le dictionnaire en une chaîne de caractères, comme dans le script
    input_str = str(input_for_agent_2)
    print(f"   > Input for Agent 2: {input_str}")
    
    try:
        resp_2 = client.beta.conversations.start(
            agent_id=AGENT_ID_QUESTIONER,
            inputs=[{"role": "user", "content": input_str}],
            store=False
        )
        
        question = resp_2.outputs[0].content if resp_2.outputs else "Sorry, I'm missing some information but I'm not sure what to ask."
        print(f"   > Question generated by Agent 2: {question}")
        
        return {
            "messages": [AIMessage(content=question)],
            "question_attempts": state.get('question_attempts', 0) + 1
        }
    except Exception as e:
        print(f"   > Error calling Mistral Agent 2: {e}")
        return {"messages": [AIMessage(content="I'm having trouble thinking of the right question to ask. Could you please provide more details?")]}

# =============================================================================
# ÉTAPE 4 : ROUTAGE ET ASSEMBLAGE
# =============================================================================

def decide_next_step(state: GraphState) -> Literal["ask_clarification_question", "END"]:
    """Décide s'il faut continuer à poser des questions ou si le profil est complet."""
    print("--- ROUTER: Deciding Next Step ---")
    # La logique ici reste la même : on se base sur la présence de `missing_fields`
    if state.get('user_profile', {}).get('missing_fields'):
        print("   > Decision: Profile incomplete. Asking for clarification.")
        return "ask_clarification_question"
    else:
        print("   > Decision: Profile complete. Ready for next phase.")
        return "END"

# --- Assemblage ---
builder = StateGraph(GraphState)
builder.add_node("information_extractor", information_extractor_node)
builder.add_node("ask_clarification_question", ask_clarification_question_node)
builder.set_entry_point("information_extractor")
builder.add_conditional_edges(
    "information_extractor",
    decide_next_step,
    {"ask_clarification_question": "ask_clarification_question", "END": END}
)
builder.add_edge("ask_clarification_question", "information_extractor")

checkpointer = InMemorySaver()
le_guide_royal = builder.compile(checkpointer=checkpointer)
print("\n--- Graph with Mistral Agent Wrappers Compiled Successfully ---")



